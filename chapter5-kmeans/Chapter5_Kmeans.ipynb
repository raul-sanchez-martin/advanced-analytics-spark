{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Anomaly Detection in Network Traffic with K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{PipelineModel, Pipeline}\n",
    "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.ml.feature.{OneHotEncoder, VectorAssembler, StringIndexer, StandardScaler}\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First Take on Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_path = ../data/kmeans/kddcup.data\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "../data/kmeans/kddcup.data"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_path = \"../data/kmeans/kddcup.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = [duration: int, protocol_type: string ... 40 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[duration: int, protocol_type: string ... 40 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.read.\n",
    "      option(\"inferSchema\", true).\n",
    "      option(\"header\", false).\n",
    "      csv(data_path).\n",
    "      toDF(\n",
    "        \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "        \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "        \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "        \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "        \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "        \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "        \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "        \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "        \"dst_host_count\", \"dst_host_srv_count\",\n",
    "        \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "        \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "        \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "        \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "        \"label\")\n",
    "\n",
    "    data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|           label| count|\n",
      "+----------------+------+\n",
      "|          smurf.|280790|\n",
      "|        neptune.|107201|\n",
      "|         normal.| 97278|\n",
      "|           back.|  2203|\n",
      "|          satan.|  1589|\n",
      "|        ipsweep.|  1247|\n",
      "|      portsweep.|  1040|\n",
      "|    warezclient.|  1020|\n",
      "|       teardrop.|   979|\n",
      "|            pod.|   264|\n",
      "|           nmap.|   231|\n",
      "|   guess_passwd.|    53|\n",
      "|buffer_overflow.|    30|\n",
      "|           land.|    21|\n",
      "|    warezmaster.|    20|\n",
      "|           imap.|    12|\n",
      "|        rootkit.|    10|\n",
      "|     loadmodule.|     9|\n",
      "|      ftp_write.|     8|\n",
      "|       multihop.|     7|\n",
      "|            phf.|     4|\n",
      "|           perl.|     3|\n",
      "|            spy.|     2|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"label\").groupBy(\"label\").count().orderBy($\"count\".desc).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.979395571029514,1622.078830816566,868.5341828266062,4.453261001578883E-5,0.006432937937735314,1.4169466823205539E-5,0.03451682118132869,1.5181571596291647E-4,0.14824703453301485,0.01021213716043885,1.1133152503947209E-4,3.6435771831099954E-5,0.011351767134933808,0.0010829521072021374,1.0930731549329986E-4,0.0010080563539937655,0.0,0.0,0.0013865835391279706,332.2862475203433,292.9071434354884,0.17668541759442963,0.17660780940042922,0.05743309987449894,0.05771839196793656,0.7915488441763401,0.020981640419415717,0.028996862475203753,232.4707319541719,188.6660459090725,0.7537812031901896,0.03090561110887087,0.6019355289259497,0.0066835148374549125,0.17675395732965926,0.17644162179668316,0.05811762681672753,0.05741111695882672]\n",
      "[2.0,6.9337564E8,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,57.0,3.0,0.79,0.67,0.21,0.33,0.05,0.39,0.0,255.0,3.0,0.01,0.09,0.22,0.0,0.18,0.67,0.05,0.33]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numericOnly = [duration: int, src_bytes: int ... 37 more fields]\n",
       "assembler = vecAssembler_e76dc4db5687\n",
       "kmeans = kmeans_204a3e5834f8\n",
       "pipeline = pipeline_adc070594d48\n",
       "pipelineModel = pipeline_adc070594d48\n",
       "kmeansModel = kmeans_204a3e5834f8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "kmeans_204a3e5834f8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n",
    "\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(numericOnly.columns.filter(_ != \"label\")).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val kmeans = new KMeans().b\n",
    "    setSeed(42).\n",
    "    setPredictionCol(\"cluster\").\n",
    "    setFeaturesCol(\"featureVector\")\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, kmeans))\n",
    "val pipelineModel = pipeline.fit(numericOnly)\n",
    "val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "\n",
    "kmeansModel.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+\n",
      "|cluster|           label| count|\n",
      "+-------+----------------+------+\n",
      "|      0|          smurf.|280790|\n",
      "|      0|        neptune.|107201|\n",
      "|      0|         normal.| 97278|\n",
      "|      0|           back.|  2203|\n",
      "|      0|          satan.|  1589|\n",
      "|      0|        ipsweep.|  1247|\n",
      "|      0|      portsweep.|  1039|\n",
      "|      0|    warezclient.|  1020|\n",
      "|      0|       teardrop.|   979|\n",
      "|      0|            pod.|   264|\n",
      "|      0|           nmap.|   231|\n",
      "|      0|   guess_passwd.|    53|\n",
      "|      0|buffer_overflow.|    30|\n",
      "|      0|           land.|    21|\n",
      "|      0|    warezmaster.|    20|\n",
      "|      0|           imap.|    12|\n",
      "|      0|        rootkit.|    10|\n",
      "|      0|     loadmodule.|     9|\n",
      "|      0|      ftp_write.|     8|\n",
      "|      0|       multihop.|     7|\n",
      "|      0|            phf.|     4|\n",
      "|      0|           perl.|     3|\n",
      "|      0|            spy.|     2|\n",
      "|      1|      portsweep.|     1|\n",
      "+-------+----------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "withCluster = [duration: int, src_bytes: int ... 39 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[duration: int, src_bytes: int ... 37 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withCluster = pipelineModel.transform(numericOnly)\n",
    "\n",
    "withCluster.select(\"cluster\", \"label\").\n",
    "    groupBy(\"cluster\", \"label\").count().\n",
    "    orderBy($\"cluster\", $\"count\".desc).\n",
    "    show(25)\n",
    "\n",
    "numericOnly.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusteringScore0: (data: org.apache.spark.sql.DataFrame, k: Int)Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clusteringScore0(data: DataFrame, k: Int): Double = {\n",
    "    val assembler = new VectorAssembler().\n",
    "      setInputCols(data.columns.filter(_ != \"label\")).\n",
    "      setOutputCol(\"featureVector\")\n",
    "\n",
    "    val kmeans = new KMeans().\n",
    "      setSeed(42).\n",
    "      setK(k).\n",
    "      setPredictionCol(\"cluster\").\n",
    "      setFeaturesCol(\"featureVector\")\n",
    "\n",
    "    val pipeline = new Pipeline().setStages(Array(assembler, kmeans))\n",
    "\n",
    "    val kmeansModel = pipeline.fit(data).stages.last.asInstanceOf[KMeansModel]\n",
    "    kmeansModel.computeCost(assembler.transform(data)) / data.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusteringScore1: (data: org.apache.spark.sql.DataFrame, k: Int)Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clusteringScore1(data: DataFrame, k: Int): Double = {\n",
    "    val assembler = new VectorAssembler().\n",
    "      setInputCols(data.columns.filter(_ != \"label\")).\n",
    "      setOutputCol(\"featureVector\")\n",
    "\n",
    "    val kmeans = new KMeans().\n",
    "      setSeed(Random.nextLong()).\n",
    "      setK(k).\n",
    "      setPredictionCol(\"cluster\").\n",
    "      setFeaturesCol(\"featureVector\").\n",
    "      setMaxIter(40).\n",
    "      setTol(1.0e-5)\n",
    "\n",
    "    val pipeline = new Pipeline().setStages(Array(assembler, kmeans))\n",
    "\n",
    "    val kmeansModel = pipeline.fit(data).stages.last.asInstanceOf[KMeansModel]\n",
    "    kmeansModel.computeCost(assembler.transform(data)) / data.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numericOnly = [duration: int, src_bytes: int ... 37 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[duration: int, src_bytes: int ... 37 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,6.988910071522829E7)\n",
      "(40,6.98890952256968E7)\n",
      "(60,3.223287575527558E7)\n",
      "(80,3.155325456382279E7)\n",
      "(100,2.6254419772138733E7)\n"
     ]
    }
   ],
   "source": [
    "(20 to 100 by 20).map(k => (k, clusteringScore0(numericOnly, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,3.057847974935887E8)\n",
      "(40,4.4047195387869634E7)\n",
      "(60,3.4495793501899E7)\n",
      "(80,1.1607611352200137E7)\n",
      "(100,4644073.062761333)\n"
     ]
    }
   ],
   "source": [
    "(20 to 100 by 20).map(k => (k, clusteringScore1(numericOnly, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[duration: int, src_bytes: int ... 37 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericOnly.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusteringScore2: (data: org.apache.spark.sql.DataFrame, k: Int)Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clusteringScore2(data: DataFrame, k: Int): Double = {\n",
    "    val assembler = new VectorAssembler().\n",
    "      setInputCols(data.columns.filter(_ != \"label\")).\n",
    "      setOutputCol(\"featureVector\")\n",
    "\n",
    "    val scaler = new StandardScaler()\n",
    "      .setInputCol(\"featureVector\")\n",
    "      .setOutputCol(\"scaledFeatureVector\")\n",
    "      .setWithStd(true)\n",
    "      .setWithMean(false)\n",
    "\n",
    "    val kmeans = new KMeans().\n",
    "      setSeed(Random.nextLong()).\n",
    "      setK(k).\n",
    "      setPredictionCol(\"cluster\").\n",
    "      setFeaturesCol(\"scaledFeatureVector\").\n",
    "      setMaxIter(40).\n",
    "      setTol(1.0e-5)\n",
    "\n",
    "    val pipeline = new Pipeline().setStages(Array(assembler, scaler, kmeans))\n",
    "    val pipelineModel = pipeline.fit(data)\n",
    "\n",
    "    val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "    kmeansModel.computeCost(pipelineModel.transform(data)) / data.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[duration: int, src_bytes: int ... 37 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60,1.2239030219540843)\n",
      "(90,0.7792502727047937)\n",
      "(120,0.49406744648386625)\n",
      "(150,0.37671225612994114)\n",
      "(180,0.3093477205019793)\n",
      "(210,0.26749170699958735)\n",
      "(240,0.22614663308865912)\n",
      "(270,0.20291296647759655)\n"
     ]
    }
   ],
   "source": [
    "(60 to 270 by 30).map(k => (k, clusteringScore2(numericOnly, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericOnly.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n",
       "oneHotPipeline: (inputCol: String)(org.apache.spark.ml.Pipeline, String)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def oneHotPipeline(inputCol: String): (Pipeline, String) = {\n",
    "    val indexer = new StringIndexer().\n",
    "      setInputCol(inputCol).\n",
    "      setOutputCol(inputCol + \"_indexed\")\n",
    "    val encoder = new OneHotEncoder().\n",
    "      setInputCol(inputCol + \"_indexed\").\n",
    "      setOutputCol(inputCol + \"_vec\")\n",
    "    val pipeline = new Pipeline().setStages(Array(indexer, encoder))\n",
    "    (pipeline, inputCol + \"_vec\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "protoTypeEncoder = pipeline_8a510b6512ca\n",
       "protoTypeVecCol = protocol_type_vec\n",
       "serviceEncoder = pipeline_693fe71cce72\n",
       "serviceVecCol = service_vec\n",
       "flagEncoder = pipeline_4402a0adcf91\n",
       "flagVecCol = flag_vec\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "flag_vec"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
    "val (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
    "val (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val assembleCols = Set(data.columns: _*) --\n",
    "    Seq(\"label\", \"protocol_type\", \"service\", \"flag\") ++\n",
    "    Seq(protoTypeVecCol, serviceVecCol, flagVecCol)\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(assembleCols.toArray).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "    .setInputCol(\"featureVector\")\n",
    "    .setOutputCol(\"scaledFeatureVector\")\n",
    "    .setWithStd(true)\n",
    "    .setWithMean(false)\n",
    "\n",
    "val kmeans = new KMeans().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setK(k).\n",
    "    setPredictionCol(\"cluster\").\n",
    "    setFeaturesCol(\"scaledFeatureVector\").\n",
    "    setMaxIter(40).\n",
    "    setTol(1.0e-5)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(\n",
    "    Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel = pipeline_adc070594d48\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_adc070594d48"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelineModel = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "kmeansModel.computeCost(pipelineModel.transform(data)) / data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Labels with Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entropy: (counts: Iterable[Int])Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def entropy(counts: Iterable[Int]): Double = {\n",
    "    val values = counts.filter(_ > 0)\n",
    "    val n = values.map(_.toDouble).sum\n",
    "    values.map { v =>\n",
    "      val p = v / n\n",
    "      -p * math.log(p)\n",
    "    }.sum\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fitPipeline4: (data: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fitPipeline4(data: DataFrame, k: Int): PipelineModel = {\n",
    "    val (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
    "    val (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
    "    val (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")\n",
    "\n",
    "    // Original columns, without label / string columns, but with new vector encoded cols\n",
    "    val assembleCols = Set(data.columns: _*) --\n",
    "      Seq(\"label\", \"protocol_type\", \"service\", \"flag\") ++\n",
    "      Seq(protoTypeVecCol, serviceVecCol, flagVecCol)\n",
    "    val assembler = new VectorAssembler().\n",
    "      setInputCols(assembleCols.toArray).\n",
    "      setOutputCol(\"featureVector\")\n",
    "\n",
    "    val scaler = new StandardScaler()\n",
    "      .setInputCol(\"featureVector\")\n",
    "      .setOutputCol(\"scaledFeatureVector\")\n",
    "      .setWithStd(true)\n",
    "      .setWithMean(false)\n",
    "\n",
    "    val kmeans = new KMeans().\n",
    "      setSeed(Random.nextLong()).\n",
    "      setK(k).\n",
    "      setPredictionCol(\"cluster\").\n",
    "      setFeaturesCol(\"scaledFeatureVector\").\n",
    "      setMaxIter(40).\n",
    "      setTol(1.0e-5)\n",
    "\n",
    "    val pipeline = new Pipeline().setStages(\n",
    "      Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans))\n",
    "    pipeline.fit(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusteringScore4: (data: org.apache.spark.sql.DataFrame, k: Int)Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clusteringScore4(data: DataFrame, k: Int): Double = {\n",
    "    val pipelineModel = fitPipeline4(data, k)\n",
    "\n",
    "    // Predict cluster for each datum\n",
    "    val clusterLabel = pipelineModel.transform(data).\n",
    "      select(\"cluster\", \"label\").as[(Int, String)]\n",
    "    val weightedClusterEntropy = clusterLabel.\n",
    "      // Extract collections of labels, per cluster\n",
    "      groupByKey { case (cluster, _) => cluster }.\n",
    "      mapGroups { case (_, clusterLabels) =>\n",
    "        val labels = clusterLabels.map { case (_, label) => label }.toSeq\n",
    "        // Count labels in collections\n",
    "        val labelCounts = labels.groupBy(identity).values.map(_.size)\n",
    "        labels.size * entropy(labelCounts)\n",
    "      }.collect()\n",
    "\n",
    "    // Average entropy weighted by cluster size\n",
    "    weightedClusterEntropy.sum / data.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusteringTake4: (data: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clusteringTake4(data: DataFrame): Unit = {\n",
    "    (60 to 270 by 30).map(k => (k, clusteringScore4(data, k))).foreach(println)\n",
    "\n",
    "    val pipelineModel = fitPipeline4(data, 180)\n",
    "    val countByClusterLabel = pipelineModel.transform(data).\n",
    "      select(\"cluster\", \"label\").\n",
    "      groupBy(\"cluster\", \"label\").count().\n",
    "      orderBy(\"cluster\", \"label\")\n",
    "    countByClusterLabel.show()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[duration: int, protocol_type: string ... 40 more fields]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60,0.04942426371114013)\n",
      "(90,0.049396769747763844)\n",
      "(120,0.04312248060837521)\n",
      "(150,0.038291546744763545)\n",
      "(180,0.022577093557218056)\n",
      "(210,0.019398614828370613)\n",
      "(240,0.009805853286683406)\n",
      "(270,0.020184799437952816)\n",
      "+-------+----------+-----+\n",
      "|cluster|     label|count|\n",
      "+-------+----------+-----+\n",
      "|      0|  neptune.|36151|\n",
      "|      1|   normal.|21789|\n",
      "|      2|  neptune.|  106|\n",
      "|      2|portsweep.|    1|\n",
      "|      3|     imap.|    7|\n",
      "|      3|  neptune.|  105|\n",
      "|      4|  neptune.|  102|\n",
      "|      4|portsweep.|    1|\n",
      "|      4|    satan.|    1|\n",
      "|      5|   normal.|    1|\n",
      "|      6|     back.|    4|\n",
      "|      6|   normal.|18699|\n",
      "|      7|  neptune.|   16|\n",
      "|      7|portsweep.|    2|\n",
      "|      7|    satan.|    1|\n",
      "|      8|  neptune.|  105|\n",
      "|      9|  ipsweep.|    1|\n",
      "|      9|  neptune.|  118|\n",
      "|      9|portsweep.|    1|\n",
      "|     10|  neptune.|  102|\n",
      "+-------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clusteringTake4(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[duration: int, protocol_type: string ... 40 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[duration: int, protocol_type: string ... 40 more fields]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildAnomalyDetector: (data: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def buildAnomalyDetector(data: DataFrame): Unit = {\n",
    "    val pipelineModel = fitPipeline4(data, 180)\n",
    "\n",
    "    val kMeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "    val centroids = kMeansModel.clusterCenters\n",
    "\n",
    "    val clustered = pipelineModel.transform(data)\n",
    "    val threshold = clustered.\n",
    "      select(\"cluster\", \"scaledFeatureVector\").as[(Int, Vector)].\n",
    "      map { case (cluster, vec) => Vectors.sqdist(centroids(cluster), vec) }.\n",
    "      orderBy($\"value\".desc).take(100).last\n",
    "\n",
    "    val originalCols = data.columns\n",
    "    val anomalies = clustered.filter { row =>\n",
    "      val cluster = row.getAs[Int](\"cluster\")\n",
    "      val vec = row.getAs[Vector](\"scaledFeatureVector\")\n",
    "      Vectors.sqdist(centroids(cluster), vec) >= threshold\n",
    "    }.select(originalCols.head, originalCols.tail:_*)\n",
    "\n",
    "    println(anomalies.first())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,tcp,ftp,SF,60,189,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0.0,0.0,0.0,0.0,1.0,0.0,0.0,151,47,0.31,0.03,0.01,0.0,0.0,0.0,0.0,0.0,normal.]\n"
     ]
    }
   ],
   "source": [
    "buildAnomalyDetector(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[duration: int, protocol_type: string ... 40 more fields]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
