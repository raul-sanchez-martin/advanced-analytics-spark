{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Forest Cover with Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{functions => F}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data_path = \"../data/covtype/covtype.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dataWithoutHeader = spark.read.option(\"header\", \"false\").\n",
    "    option(\"inferSchema\", \"true\").csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val colNames = Seq(\n",
    "        \"Elevation\", \"Aspect\", \"Slope\",\n",
    "        \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n",
    "        \"Horizontal_Distance_To_Roadways\",\n",
    "        \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "        \"Horizontal_Distance_To_Fire_Points\"\n",
    "      ) ++ (\n",
    "        (0 until 4).map(i => s\"Wilderness_Area_$i\")\n",
    "      ) ++ (\n",
    "        (0 until 40).map(i => s\"Soil_Type_$i\")\n",
    "      ) ++ Seq(\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = dataWithoutHeader.toDF(colNames:_*).withColumn(\"Cover_Type\", F.col(\"Cover_type\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A First Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(trainData, testData) = data.randomSplit(Array(0.9, 0.1))\n",
    "trainData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val inputCols = trainData.columns.filter(_ != \"Cover_Type\")\n",
    "val assembler = new VectorAssembler().setInputCols(inputCols)\n",
    "    .setOutputCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val assembledTrainData = assembler.transform(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembledTrainData.select(\"featureVector\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val classifier = new DecisionTreeClassifier().\n",
    "    setSeed(42).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"featureVector\").\n",
    "    setPredictionCol(\"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val model = classifier.fit(assembledTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "println(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.featureImportances.toArray.zip(inputCols).sorted.reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val predictions = model.transform(assembledTrainData)\n",
    "predictions.select(\"Cover_Type\", \"prediction\", \"probability\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.setMetricName(\"f1\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictionRDD = predictions.select(\"prediction\", \"Cover_Type\").as[(Double, Double)].rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val multiclassMetrics = new MulticlassMetrics(predictionRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclassMetrics.confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val confusionMatrix = predictions.select(\"prediction\", \"Cover_Type\")\n",
    "    .groupBy(\"Cover_Type\").pivot(\"prediction\").count().na.fill(0).orderBy(\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classProbabilities(data: DataFrame): Array[Double] = {\n",
    "    val total = data.count()\n",
    "    data.groupBy(\"Cover_Type\").count().\n",
    "      orderBy(\"Cover_Type\").\n",
    "      select(\"count\").as[Double].\n",
    "      map(_ / total).\n",
    "      collect()\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trainPriorProbabilities = classProbabilities(trainData)\n",
    "val testPriorProbabilities = classProbabilities(testData)\n",
    "trainPriorProbabilities.zip(testPriorProbabilities).map{\n",
    "    case(trainProb, cvProb) => trainProb * cvProb\n",
    "}.sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val inputCols = trainData.columns.filter(_ != \"Cover_Type\")\n",
    "\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(inputCols).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val classifier = new DecisionTreeClassifier().\n",
    "    setSeed(42).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"featureVector\").\n",
    "    setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pipeline = new Pipeline().setStages(Array(assembler, classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, \n",
    "                                   TrainValidationSplit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val paramGrid = new ParamGridBuilder().\n",
    "      addGrid(classifier.impurity, Seq(\"gini\", \"entropy\")).\n",
    "      addGrid(classifier.maxDepth, Seq(1, 20)).\n",
    "      build()\n",
    "\n",
    "val multiclassEval = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\").\n",
    "    setMetricName(\"accuracy\")\n",
    "\n",
    "val validator = new TrainValidationSplit().\n",
    "    setSeed(42).\n",
    "    setEstimator(pipeline).\n",
    "    setEvaluator(multiclassEval).\n",
    "    setEstimatorParamMaps(paramGrid).\n",
    "    setTrainRatio(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val validatorModel = validator.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val paramsAndMetrics = validatorModel.validationMetrics.\n",
    "      zip(validatorModel.getEstimatorParamMaps).sortBy(-_._1)\n",
    "\n",
    "paramsAndMetrics.foreach { case (metric, params) =>\n",
    "    println(metric)\n",
    "    println(params)\n",
    "    println()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val bestModel = validatorModel.bestModel\n",
    "println(bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(validatorModel.validationMetrics.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testAccuracy = multiclassEval.evaluate(bestModel.transform(testData))\n",
    "println(testAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trainAccuracy = multiclassEval.evaluate(bestModel.transform(trainData))\n",
    "println(trainAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features Revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unencodeOneHot(data: DataFrame): DataFrame = {\n",
    "    val wildernessCols = (0 until 4).map(i => s\"Wilderness_Area_$i\").toArray\n",
    "\n",
    "    val wildernessAssembler = new VectorAssembler().\n",
    "      setInputCols(wildernessCols).\n",
    "      setOutputCol(\"wilderness\")\n",
    "\n",
    "    val unhotUDF = udf((vec: Vector[Double]) => vec.toArray.indexOf(1.0).toDouble)\n",
    "\n",
    "    val withWilderness = wildernessAssembler.transform(data).\n",
    "      drop(wildernessCols:_*).\n",
    "      withColumn(\"wilderness\", unhotUDF($\"wilderness\"))\n",
    "\n",
    "    val soilCols = (0 until 40).map(i => s\"Soil_Type_$i\").toArray\n",
    "\n",
    "    val soilAssembler = new VectorAssembler().\n",
    "      setInputCols(soilCols).\n",
    "      setOutputCol(\"soil\")\n",
    "\n",
    "    soilAssembler.transform(withWilderness).\n",
    "      drop(soilCols:_*).\n",
    "      withColumn(\"soil\", unhotUDF($\"soil\"))\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// val unencTrainData = unencodeOneHot(trainData)\n",
    "// val unencTestData = unencodeOneHot(testData)\n",
    "\n",
    "// val inputCols = unencTrainData.columns.filter(_ != \"Cover_Type\")\n",
    "// val assembler = new VectorAssembler().\n",
    "//     setInputCols(inputCols).\n",
    "//     setOutputCol(\"featureVector\")\n",
    "\n",
    "// val indexer = new VectorIndexer().\n",
    "//     setMaxCategories(40).\n",
    "//     setInputCol(\"featureVector\").\n",
    "//     setOutputCol(\"indexedVector\")\n",
    "\n",
    "// val classifier = new DecisionTreeClassifier().\n",
    "//     setSeed(42).\n",
    "//     setLabelCol(\"Cover_Type\").\n",
    "//     setFeaturesCol(\"indexedVector\").\n",
    "//     setPredictionCol(\"prediction\")\n",
    "\n",
    "// val pipeline = new Pipeline().setStages(Array(assembler, indexer, classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Decision Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val classifier = new RandomForestClassifier().\n",
    "      setSeed(42).\n",
    "      setLabelCol(\"Cover_Type\").\n",
    "      setFeaturesCol(\"featureVector\").\n",
    "      setPredictionCol(\"prediction\").\n",
    "      setImpurity(\"entropy\").\n",
    "      setMaxDepth(20).\n",
    "      setMaxBins(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pipeline = new Pipeline().setStages(Array(assembler, classifier))\n",
    "\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "    addGrid(classifier.minInfoGain, Seq(0.0, 0.05)).\n",
    "    build()\n",
    "\n",
    "val multiclassEval = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\").\n",
    "    setMetricName(\"accuracy\")\n",
    "\n",
    "val validator = new TrainValidationSplit().\n",
    "    setSeed(42).\n",
    "    setEstimator(pipeline).\n",
    "    setEvaluator(multiclassEval).\n",
    "    setEstimatorParamMaps(paramGrid).\n",
    "    setTrainRatio(0.9)\n",
    "\n",
    "val validatorModel = validator.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val bestModel = validatorModel.bestModel\n",
    "\n",
    "val forestModel = bestModel.asInstanceOf[PipelineModel].\n",
    "    stages.last.asInstanceOf[RandomForestClassificationModel]\n",
    "\n",
    "println(forestModel.extractParamMap)\n",
    "println(forestModel.getNumTrees)\n",
    "forestModel.featureImportances.toArray.zip(inputCols).\n",
    "    sorted.reverse.foreach(println)\n",
    "\n",
    "val testAccuracy = multiclassEval.evaluate(bestModel.transform(testData))\n",
    "println(testAccuracy)\n",
    "\n",
    "bestModel.transform(unencTestData.drop(\"Cover_Type\")).select(\"prediction\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
